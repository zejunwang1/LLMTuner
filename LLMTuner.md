# LLMTuner: 大语言模型指令调优工具（支持全量参数微调、LoRA 和 QLoRA）

## News

- 开源在 12w 条多轮对话数据上全量参数微调得到的 bloom-3b-moss-chat 

- 使用 QLoRA 技术，在 12w 条多轮对话数据上微调得到的 bloomz-7b1-qlora-moss-chat  

- 开源全量参数训练、LoRA 和 QLoRA 训练代码。

## 项目简介

LLMTuner 是一个支持全量参数微调、LoRA 和 QLoRA 的大语言模型指令调优工具。在训练中可以结合词表裁剪、DeepSpeed-ZeRO、gradient checkpointing、4-bit 量化等技术，有效降低显存占用，实现在单张消费级显卡上微调 7B/13B/33B 大模型。

我们从 MOSS 项目开源的中英文指令微调数据集 [moss-003-sft-data](https://huggingface.co/datasets/fnlp/moss-003-sft-data) 中抽取 12w 条多轮对话数据作为训练数据，分别：

- 以词表裁剪后的 bloom-3b 为基座，全量参数微调得到 bloom-3b-moss-chat

- 以词表裁剪后的 bloomz-7b1-mt 为基座，使用 QLoRA 技术微调得到 bloomz-7b1-qlora-moss-chat

## Requirements

- bitsandbytes==0.39.0

- transformers @ git+https://github.com/huggingface/transformers.git

- peft @ git+https://github.com/huggingface/peft.git

- accelerate @ git+https://github.com/huggingface/accelerate.git

- tensorboardX

```shell
pip install -U -r requirements.txt
```

## 训练数据

训练数据中的单条指令需要预处理为如下形式：

```json
{
    "conversation_id": 4, 
    "category": "Brainstorming", 
    "num_turns": 4, 
    "conversation": [
        {
            "human": "如何制定一个有效的网页设计？", 
            "assistant": "\n\n要制定一个有效的网页设计，需要考虑以下几个方面：\n\n1. 目标受众：确定您的目标受众是谁，这将有助于您决定网站的设计和内容。\n\n2. 清晰的布局：网站的布局应该简单，易于使用和导航。所有元素应该有明确的标识和位置，以便用户快速找到他们需要的信息。\n\n3. 吸引人的视觉效果：网站的设计应该吸引人，使用清晰的图像、颜色和字体。但同时也要确保视觉效果不会影响网站的性能和导航。\n\n4. 有效的内容：网站的内容应该有价值，易于理解和使用。使用简单的语言和段落，避免使用过于复杂的术语和语言。\n\n5. 可访问性：网站应该易于访问，即使是对于有视觉或听力障碍的用户。确保网站符合标准的可访问性要求。\n\n6. 响应式设计：网站应该是响应式的，即适合各种不同尺寸的设备。这将确保用户在不同设备上都能够访问和使用您的网站。\n\n通过考虑这些方面，您可以制定一个有效的网页设计，吸引并留住您的受众，并提高您的网站的成功率。"
        }, 
        {
            "human": "哪些因素会影响网站的性能？", 
            "assistant": "网站的性能受许多因素的影响，以下是一些主要的因素：\n\n1. 服务器性能：网站的速度和性能取决于服务器的性能，包括硬件、软件、配置和网络连接速度。\n\n2. 图片和媒体文件的大小：大尺寸的图片和媒体文件会增加网站的加载时间，因此需要使用适当的图片和媒体文件大小，以确保网站可以快速加载。\n\n3. 代码优化：网站的代码应尽可能简洁和优化，以确保它在加载和运行时的速度最快。\n\n4. 缓存和压缩：使用缓存和压缩技术可以大大提高网站的性能，因为它们可以减少文件大小和加载时间。\n\n5. 第三方插件和脚本：第三方插件和脚本可能会减慢网站的速度，因此需要谨慎选择并仅使用必要的插件和脚本。\n\n6. 浏览器兼容性：网站应该在各种不同的浏览器上兼容，因为不同的浏览器可能会以不同的方式解释代码并表现出不同的性能。\n\n7. 网络连接速度：用户的网络连接速度也会影响网站的性能，如果用户的网络连接速度较慢，网站的加载时间可能会更长。\n\n总之，要提高网站的性能，需要优化服务器、代码和媒体文件，使用缓存和压缩技术，谨慎使用第三方插件和脚本，并考虑用户的网络连接速度和浏览器兼容性。"
        }, 
        {
            "human": "如何确定网站的目标受众？", 
            "assistant": "确定网站的目标受众是成功的关键之一，以下是确定目标受众的一些步骤：\n\n1. 考虑您的业务：首先，您需要考虑您的业务或服务类型，以及您想要吸引的客户或用户类型。比如，如果您是一家专门从事儿童玩具销售的公司，您的目标受众可能是父母或祖父母。\n\n2. 调查市场：进行市场调查，了解您的目标受众的需求、喜好、行为和偏好。您可以使用各种市场调查技术，如在线调查、焦点小组讨论等等。\n\n3. 竞争分析：研究竞争对手的网站，了解他们的目标受众、品牌声誉和营销策略。\n\n4. 客户反馈：收集客户反馈，了解他们对您网站的看法，以及您的网站是否满足他们的需求和期望。\n\n5. 利用分析工具：使用分析工具（如Google Analytics）了解您的网站的访问者特征，比如年龄、性别、地理位置等，从而了解您的目标受众。\n\n综上所述，确定网站的目标受众需要考虑您的业务类型、市场调查、竞争分析、客户反馈和分析工具。这些步骤可以帮助您更好地了解您的目标受众，从而制定更好的网站设计和营销策略。"
        }, 
        {
            "human": "如何在网站上提供更好的用户体验？", 
            "assistant": "提供更好的用户体验是一个成功的网站设计的关键。以下是一些提高网站用户体验的方法：\n\n1. 清晰的页面布局：网站应该有清晰的页面布局，易于使用和导航。所有元素应该有明确的标识和位置，以便用户快速找到他们需要的信息。\n\n2. 易于理解和使用的导航：网站的导航应该简单、明确和易于理解。用户应该能够快速找到他们需要的信息，而不必浪费时间在网站上寻找。\n\n3. 快速的网站加载速度：网站的加载速度是提供良好用户体验的关键因素之一。网站应该尽可能快地加载，以避免用户等待过长的时间。\n\n4. 响应式设计：网站应该是响应式的，即适合各种不同尺寸的设备。这将确保用户在不同设备上都能够访问和使用您的网站。\n\n5. 良好的内容：网站的内容应该有价值，易于理解和使用。使用简单的语言和段落，避免使用过于复杂的术语和语言。\n\n6. 与用户互动：网站应该与用户互动，以吸引和留住用户。例如，您可以添加评论部分、交互式表单和调查等功能。\n\n7. 提供搜索功能：对于较大的网站，提供搜索功能可以帮助用户快速找到他们需要的信息。\n\n8. 避免繁琐的注册流程：网站应该尽可能避免繁琐的注册流程，以避免阻碍用户的使用。\n\n9. 优化网站的可访问性：网站应该易于访问，即使是对于有视觉或听力障碍的用户。确保网站符合标准的可访问性要求。\n\n总之，提供更好的用户体验需要考虑页面布局、导航、加载速度、响应式设计、内容、互动、搜索功能、注册流程和可访问性。通过优化这些方面，您可以提高用户满意度，增加用户留存率，并提高您的网站的成功率。"
        }
    ]
}
```

## 全量参数微调

基于 DeepSpeed ZeRO Stage 3 的单卡训练：

```shell
deepspeed --include localhost:0 train.py \
    --model_name_or_path /path/to/bloom \
    --data_path data/train.jsonl \
    --output_dir output/bloom-3b-moss-chat/ \
    --max_length 1024 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-5 \
    --num_train_epochs 1 \
    --lr_scheduler_type "cosine" \
    --warmup_steps 500 \
    --logging_steps 10 \
    --save_strategy "steps" \
    --save_steps 500 \
    --save_total_limit 1 \
    --report_to "tensorboard" \
    --bf16 True \
    --tf32 True \
    --deepspeed data/deepspeed.json
```

设置 `max_length=1024, batch_size=16, bf16=True`，单卡需要约 45G 显存对 [词表裁剪后的 bloom-3b](https://huggingface.co/YeungNLP/bloom-2b6-zh) 基座进行全量参数微调，在 12w 多轮对话数据集上训练了一个 epoch（大约 8000 steps），训练过程中的 loss 变化如下：

## LoRA

[LoRA](https://github.com/microsoft/LoRA) 的核心思想是冻结预训练模型权重，将可训练的秩分解矩阵注入 Transformer 架构的每一层，从而大大减少了下游任务的微调参数量。

LoRA 的实现流程概述如下：

- 在原始预训练语言模型 (PLM) 旁增加一个旁路，做一个先降维再升维的操作，以此来模拟所谓的本征秩 (intrinsic rank)；

- 训练的时候固定 PLM 的参数不变，只训练降维矩阵 $A$ 和升维矩阵 $B$，即优化器只优化右路的参数；

- 模型的输入、输出维度不变，左右两边共用模型的输入，输出时将 PLM 与旁路的输出叠加：$h=Wx+BAx$

- 用随机高斯分布 $N(0,\sigma^2)$ 初始化 $A$，用全零矩阵初始化 $B$。矩阵 $B$ 的全零初始化，使得在训练最开始的一段时间，右路的结果会接近于0，这样模块的输出就基本上来自于左路，也就是大模型原有参数的计算结果，这使得模型优化的初始点和原始的大模型保持一致。

使用 LoRA 进行单卡训练：

```shell
python train_lora.py \
    --model_type bloom
    --model_name_or_path /path/to/bloom \
    --data_path data/train.jsonl \
    --output_dir output/bloomz-7b1-lora-moss-chat \
    --max_length 1024 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --max_grad_norm 0.3 \
    --learning_rate 2e-4 \
    --num_train_epochs 1 \
    --lr_scheduler_type "cosine" \
    --warmup_steps 500 \
    --logging_steps 10 \
    --save_strategy "steps" \
    --save_steps 500 \
    --save_total_limit 1 \
    --report_to "tensorboard" \
    --gradient_checkpointing True \
    --lora_r 64 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --int8_training True
```

```
trainable params: 31,457,280 || all params: 6,261,878,784 || trainable%: 0.5023616886417199
```

## QLoRA

[QLoRA](https://github.com/artidoro/qlora) 是一种高效的微调方法，可以在保持完整的16位微调任务性能下，实现单个 48GB GPU 上微调 65B 参数量模型。QLoRA 通过冻结的 4-bit 量化预训练语言模型向低秩适配器(LoRA) 反向传播梯度。使用 4-bit NormalFloat (NF4) 量化、Double Quantization、Paged Optimizers、所有 Linear 层插入 adapter 等技术，QLoRA 在不牺牲性能的情况下大大节省了显存占用。

使用 QLoRA 进行单卡训练：

```shell
python train_qlora.py \
    --model_name_or_path /path/to/bloom \
    --data_path data/train.jsonl \
    --output_dir output/bloomz-7b1-qlora-moss-chat \
    --max_length 1024 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --max_grad_norm 0.3 \
    --learning_rate 2e-4 \
    --num_train_epochs 1 \
    --lr_scheduler_type "cosine" \
    --warmup_steps 500 \
    --logging_steps 10 \
    --save_strategy "steps" \
    --save_steps 500 \
    --save_total_limit 1 \
    --report_to "tensorboard" \
    --bf16 True \
    --tf32 True \
    --gradient_checkpointing True \
    --optim "paged_adamw_32bit" \
    --lora_r 64 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --bits 4 \
    --double_quant True \
    --quant_type "nf4"
```

以 [词表裁剪后的 bloomz-7b1-mt](https://huggingface.co/YeungNLP/bloomz-6b4-mt-zh) 为基座，在上面参数配置下，单卡仅需要约 12G 显存便可进行 QLoRA 微调。由于 QLoRA 在所有的全连接层处都插入了 Adapter，相同 `lora_r=64` 下其可训练的参数量多于 LoRA：

```
trainable params: 125,829,120 || all params: 3,336,351,744 || trainable%: 3.7714584568694685
```

在 12w 多轮对话数据集上训练了一个 epoch（大约 8000 steps），训练过程中的 loss 变化如下：



## 生成效果

#### bloom-3b-moss-chat

命令行单轮对话：

```shell
python chat.py \
    --model_name_or_path /path/to/model \
    --top_p 0.85 \
    --temperature 0.3 \
    --max_new_tokens 512 \
    --repetition_penalty 1.2
```

添加 `--multi_round` 可开启多轮对话。

<details><summary><b>单轮对话</b></summary>

| 用户 | Assistant |
| 晚上睡不着怎么办，有没有什么好建议                   | 失眠是很常见的问题，以下是一些可能有帮助的建议：
<br/>
<br/>1. 建立一个规律的睡眠时间表。尝试在每天相同的时间上床睡觉和起床。
<br/>2. 确保你的床铺舒适、安静且黑暗。使用窗帘或遮光布来保护眼睛免受光线干扰。
<br/>3. 在睡前放松自己，例如听轻柔的音乐或使用冥想应用程序。
<br/>4. 如果你感到困倦或者难以入睡，可以喝一杯温牛奶或茶水，以帮助身体进入深度睡眠状态。
<br/>5. 如果你有焦虑或其他压力源，可以在床上做一些深呼吸练习或进行瑜伽等放松活动。
<br/>6. 最后，如果你无法控制自己的情绪，可以使用抗抑郁药物或寻求专业心理咨询师的帮助。|


</details>

| 用户                                  | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 晚上睡不着怎么办，有没有什么好建议                   | 失眠是很常见的问题，以下是一些可能有帮助的建议：
<br/>
<br/>1. 建立一个规律的睡眠时间表。尝试在每天相同的时间上床睡觉和起床。
<br/>2. 确保你的床铺舒适、安静且黑暗。使用窗帘或遮光布来保护眼睛免受光线干扰。
<br/>3. 在睡前放松自己，例如听轻柔的音乐或使用冥想应用程序。
<br/>4. 如果你感到困倦或者难以入睡，可以喝一杯温牛奶或茶水，以帮助身体进入深度睡眠状态。
<br/>5. 如果你有焦虑或其他压力源，可以在床上做一些深呼吸练习或进行瑜伽等放松活动。
<br/>6. 最后，如果你无法控制自己的情绪，可以使用抗抑郁药物或寻求专业心理咨询师的帮助。|
| 推荐几款性价比高的手机                         | 以下是几个性价比比较高的智能手机：
<br/>
<br/>1. 华为 P40 Pro 5G版 - 这是一款价格实惠的5G旗舰，搭载了高性能处理器和强大的相机系统。它还有出色的屏幕显示效果和高质量的音响体验。
<br/>
<br/>2. 小米 10T Lite - 这是小米公司的一款入门级5G手机，拥有优秀的电池续航能力和快速充电功能。它还配备了一块大尺寸的高清屏幕和一个可靠的相机系统。
<br/>
<br/>3. OPPO A55 - 这是一款价格合理的5G手机，具有良好的设计和轻薄的外观设计。它的相机系统和操作系统也非常出色，可以满足大多数用户的需求。
<br/>
<br/>4. OnePlus 8 Pro - 这是一款高端5G手机，拥有顶级的硬件配置和多功能的摄像头系统。它的外观也很时尚，适合那些喜欢追求高质量的用户。
<br/>
<br/>5. Samsung Galaxy S20 Ultra - 这是一款豪华的5G手机，拥有非常好的相机系统和卓越的游戏性能。它的外观也很有吸引力，是一个不错的选择。                                                                                                     |
| 我该如何系统地学习机器学习?                      | 系统性地学习机器学习需要以下步骤：
<br/>
<br/>1. 了解基础知识：在开始学习之前，你需要先掌握一些基本的数学和编程概念。例如线性代数、微积分、概率论等。
<br/>
<br/>2. 选择合适的教材或课程：有很多优秀的机器学习教程和资源可供选择，包括Coursera上的《机器学习》课程、Kaggle的竞赛平台等等。你可以根据自己的需求和兴趣来选择适合自己的资源。
<br/>
<br/>3. 完成练习题：通过完成各种类型的练习题可以帮助你巩固所学知识和技能，同时也可以提高你的实践能力。
<br/>
<br/>4. 参加社区活动：参加机器学习的社区和论坛可以让你与其他人交流经验并获取反馈和建议。
<br/>
<br/>5. 利用开源项目：利用开源项目的代码库和技术文档进行研究和开发，这有助于加深对机器学习技术的理解和提高实际应用能力。
<br/>
<br/>6. 不断更新自己：随着科技的发展和社会的变化，机器学习技术也在不断发展和变化。因此，要保持持续学习的态度，及时关注最新的技术和趋势，以便跟上时代的步伐。                                                                                                |
| 最近身体发福了，我该如何进行科学地减肥，请你给我一些建议        | 当您感到体重增加时，以下是一些可以帮助您的科学减肥建议：
<br/>
<br/>1. 控制饮食：减少高热量、高脂肪和高糖分的食物摄入量。选择低热量的食品和饮料，如水果、蔬菜、全麦面包等。
<br/>
<br/>2. 多运动：每周至少150分钟的有氧运动可以有效地燃烧卡路里并减轻体重。可以选择跑步、游泳、骑自行车或快走等方式的运动方式。
<br/>
<br/>3. 睡眠充足：保持充足的睡眠时间有助于维持身体的代谢水平和提高新陈代谢率。
<br/>
<br/>4. 管理压力：长期的压力会导致荷尔蒙失调，从而影响食欲和消化系统。尝试使用放松技巧来缓解压力，例如冥想、瑜伽或者深呼吸练习。
<br/>
<br/>5. 建立健康的生活习惯：戒烟限酒、避免过度饮酒以及限制咖啡因的摄入都是有益于健康的减肥方法。
<br/>
<br/>请注意，每个人的情况都不同，因此最好咨询专业医生的意见以制定适合自己的减肥计划。                                                                                                                                                            |
| 如何成为一名全栈工程师?                        | 要成为一个全栈工程师，需要具备以下技能和知识：
<br/>
<br/>1. 编程语言基础：掌握至少一种编程语言（如Java、Python等）。
<br/>2. 数据结构和算法：了解常见的数据结构（如数组、链表、树等）和常见的算法（如排序、查找、动态规划）。
<br/>3. 前端技术（HTML/CSS/JavaScript）：熟悉前端技术的开发流程和方法论（例如React、Angular、Vue.js等）。
<br/>4. 后端技术（Ruby on Rails、Django、Node.js等）：熟练使用后端框架和技术，能够处理Web应用程序的逻辑和服务请求。
<br/>5. 项目管理工具：学习并掌握项目管理和团队协作软件的使用方法，如GitHub、Slack、Trello等。
<br/>6. 持续集成和部署：学会构建、测试和发布代码的自动化过程，以及如何在生产环境中运行应用程序。
<br/>7. 跨平台支持：在不同的操作系统和平台上进行开发和维护工作时具有良好的兼容性和稳定性。 
<br/>
<br/>以上是成为一名全栈工程师所必须的基本要求，当然还有其他方面的知识和经验也需要不断积累和提高。                                                         |
| 为新手提供制作披萨的步骤指南。                     | 当然，我很乐意为您提供关于如何制作披萨的指导。
<br/>
<br/>以下是基本的步骤：
<br/>
<br/>1. 准备面团：在开始制作之前，您需要将面粉、盐和酵母混合在一起，揉成一个光滑的面团。然后将其放在碗里静置30分钟左右，让其发酵至两倍大。
<br/>
<br/>2. 加料：现在您可以添加您的配料了。如果您喜欢番茄酱的话，可以加入一些新鲜的西红柿或罐装西红柿；如果您想要更多的蔬菜，可以选择洋葱、彩椒、蘑菇等等。在加料时要注意不要过度搅拌，以免影响口感。
<br/>
<br/>3. 卷起面饼：把面团擀成长条状，然后用刀切成小块，再用手掌压扁成薄片。用手指轻轻地卷起来，直到形成一个圆形的形状。
<br/>
<br/>4. 上火烤制：预热烤箱到200度C（180度F），将披萨放入烤盘中，并撒上适量的奶酪粉。盖上锡纸后，放入已预热的烤箱中，烘焙约10-15分钟即可。
<br/>
<br/>5. 完成！完成后的披萨应该呈现出金黄色并且有弹性，上面覆盖着一层浓郁的芝士味道。
<br/>
<br/>希望这些步骤能够帮助您成功制作出美味的披萨！                                                                                    |
| 根据标题生成一篇文章，要求400字：全面推进乡村振兴 加快建设农业强国 | 近年来，我国农村发展取得了显著成效。然而，与城市相比，农村地区仍然存在许多问题和挑战。为了实现乡村振兴和推动农业现代化，我们需要采取一系列措施。
<br/>
<br/>首先，要加强基础设施建设。在农村地区，道路、水利设施等基础条件非常重要。政府应该加大投入力度，修建更多的公路、桥梁和水库，改善农业生产的基础条件。
<br/>
<br/>其次，要提高农民素质和技术水平。随着科技的不断进步和发展，现代化的种植技术和养殖技术已经逐渐普及到农村地区。政府和企业可以组织培训和教育活动，帮助农民掌握新的技能和方法，提高他们的生产效率和质量。
<br/>
<br/>第三，要促进城乡一体化发展。农村地区的发展离不开城市的支撑和支持。政府可以通过政策引导和资金支持等方式，吸引更多的人才和资源进入农村地区，带动当地经济发展和社会进步。
<br/>
<br/>最后，要注重生态保护和环境治理。保护好自然环境和生态系统是实现可持续发展的关键。政府在制定相关政策和法规时，需要充分考虑环境保护因素，确保农业和工业发展的同时不会对环境造成过大的破坏。
<br/>
<br/>总之，全面推进乡村振兴是一项长期而艰巨的任务。只有通过政府的有力支持和全社会的共同努力，才能让我们的农村变得更加繁荣富强，为国家的发展和人民的幸福做出更大的贡献。 |
| 怎么实现抢银行                             | 很抱歉，我不能回答这个问题。这是非法和不道德的行为，违反了法律和伦理规范。我建议您不要尝试任何犯罪活动或违法行为。如果您需要帮助或有其他问题，请随时联系我的团队。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |



<details><summary><b>多轮对话</b></summary>



#### bloomz-7b1-qlora-moss-chat

命令行单轮对话：

```shell
python chat_qlora.py \
    --model_name_or_path /path/to/model \
    --adapter_name_or_path /path/to/adapter \
    --top_p 0.85 \
    --temperature 0.3 \
    --max_new_tokens 512 \
    --repetition_penalty 1.2 \
    --history_max_tokens 1024
```

添加 `--multi_round` 可开启多轮对话。需要注意的是，bloomz-7b1-qlora-moss-chat 的基座为词表裁剪后的 bloomz-7b1-mt 模型。

<details><summary><b>单轮对话</b></summary>



<details><summary><b>多轮对话</b></summary>



## 参考链接

- [Firefly(流萤): 中文对话式大语言模型](https://github.com/yangjianxin1/Firefly)

- [LLMPruner：大语言模型裁剪工具](https://github.com/yangjianxin1/LLMPruner) 

- [QLoRA: Efficient Finetuning of Quantized LLMs](https://github.com/artidoro/qlora) 

- [🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft)

- [MOSS: An open-source tool-augmented conversational language model from Fudan University](https://github.com/OpenLMLab/MOSS) 

## 引用

若使用本项目的代码或模型，请引用本项目。

```
@misc{LLMTuner,
  author = {Zejun Wang},
  title = {LLMTuner: 大语言模型指令调优工具(支持全量参数微调、LoRA 和 QLoRA)},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/zejunwang1/LLMTuner}}
}
```


























